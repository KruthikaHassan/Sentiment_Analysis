$ python3 main.py sampling.csv sad_vectors.txt 64 100 4000
Loading Train : train_dataset.pkl
train_dataset.pkl Loaded: 0.110 secs!
Loading Validation : vocab_vector.pkl
val_dataset.pkl Loaded: 0.017 secs!
Loading Test : vocab_vector.pkl
vocab_vector.pkl Loaded: 0.019 secs!
Loading Test : test_dataset.pkl
test_dataset.pkl Loaded: 0.014 secs!
Train dataset numrecords: 68000:
Validation dataset numrecords: 17000:
Test dataset numrecords: 15000:
Configuration:
epochs : 100
batchSize : 4000
lstmUnits : 64
numClasses : 2
maxSeqLength : 100
numDimensions : 25
Initializing Classifier
Classifier Initialized: 2.130 secs!
0  49.94||50.19  69.33||69.33
1  59.76||59.46  66.76||66.84
2  55.51||55.23  68.88||68.87
3  54.34||53.75  68.76||68.82
4  56.16||56.01  68.10||68.20
5  65.00||64.53  63.68||63.86
6  66.55||66.75  61.91||62.02
7  68.96||68.45  59.30||59.57
8  69.47||68.91  58.35||58.71
9  69.89||69.44  57.90||58.15
10  70.39||69.89  57.58||57.53
11  70.56||69.87  57.08||57.27
12  70.71||70.13  56.79||56.91
13  70.64||70.33  56.67||56.78
14  71.07||70.65  56.26||56.36
15  71.15||70.86  56.05||56.13
16  71.01||70.58  56.16||56.27
17  71.61||71.10  55.62||55.43
18  70.74||70.54  56.40||56.28
19  71.80||71.22  55.41||55.64
20  71.99||71.67  55.06||54.94
21  72.23||72.03  54.66||54.70
22  72.03||71.65  55.18||55.33
23  69.54||69.01  57.42||57.69
24  72.16||71.92  54.66||54.61
25  72.56||72.44  54.12||54.11
26  72.67||72.57  53.91||53.76
27  72.83||72.42  54.08||54.03
28  73.01||72.29  53.75||53.98
29  73.14||72.82  53.59||53.69
30  72.57||72.15  54.23||54.39
31  73.39||73.13  53.04||53.27
32  73.63||73.11  52.90||53.11
33  73.82||73.24  52.59||53.10
34  73.88||73.68  52.54||52.84
35  73.96||73.56  52.45||52.58
36  74.02||73.78  52.35||52.54
37  73.48||73.14  52.95||53.03
38  72.22||72.03  54.26||54.36
39  74.11||73.79  52.13||52.38
40  74.32||74.09  51.93||52.39
41  74.50||74.00  51.73||51.97
42  73.09||72.73  53.48||53.62
43  74.62||74.14  51.49||51.87
44  74.64||74.05  51.60||51.99
45  74.12||73.97  51.95||52.13
46  74.74||74.15  51.35||51.83
47  73.54||72.92  52.70||53.04
48  74.57||74.21  51.57||51.75
49  73.38||72.93  53.09||53.44
50  75.04||74.16  51.10||51.62
51  75.04||74.30  50.91||51.17
52  72.68||72.50  53.69||54.15
. 53  74.93||74.29  50.94||51.40
54  74.72||74.15  51.35||51.76
55  75.08||74.66  50.83||51.19
56  75.19||74.23  50.60||51.31
57  74.32||74.19  51.79||52.28
58  74.72||74.39  51.44||51.78
59  75.07||74.26  50.85||51.15
60  75.41||75.00  50.41||51.01
61  75.40||74.79  50.37||50.57
62  75.43||74.82  50.34||50.89
63  75.21||74.79  50.52||50.99
64  75.62||75.07  50.05||50.80
65  75.39||74.67  50.30||51.04
66  75.35||74.86  50.31||50.94
67  75.64||75.09  49.97||50.58
68  75.72||74.90  50.03||50.64
69  74.85||74.45  51.15||51.81
70  75.19||74.79  50.65||50.91
71  75.88||74.94  49.70||50.42
72  75.83||75.04  49.56||50.30
73  75.64||75.26  49.93||50.40
74  75.94||75.05  49.61||50.45
75  74.67||74.22  51.49||52.36
. 76  75.89||75.07  49.80||50.40
77  76.00||75.03  49.59||50.56
78  75.28||74.79  51.19||51.75
79  75.99||75.24  49.48||50.26
80  74.91||74.19  50.78||51.70
. 81  76.10||75.24  49.31||50.00
82  76.03||75.14  49.29||49.88
83  76.10||75.37  49.27||49.99
84  75.57||74.96  49.99||50.59
85  75.67||75.05  50.07||50.71
86  76.12||75.60  49.19||49.87
87  74.85||74.13  50.67||51.77
. 88  75.75||75.08  49.71||50.26
89  75.30||74.64  50.09||51.03
. 90  75.70||74.82  49.69||50.69
. 91  76.41||75.36  48.83||49.99
92  76.27||75.52  49.22||50.27
93  75.62||75.16  49.77||50.75
94  75.76||75.26  49.63||50.28
95  75.67||75.03  50.02||50.93
. 96  75.72||74.94  49.93||50.83
. 97  76.16||75.40  49.21||50.08
98  76.25||75.39  48.98||50.14
99  76.39||75.53  48.74||49.73

validation Status: 
 {'accuracy': 0.7567, 'precision': 0.72847456, 'recall': 0.72910386, 'f1_score': 0.7287890744689075, 'loss': 0.49648824}
Train Status: 
 {'accuracy': 0.7635, 'precision': 0.7286401, 'recall': 0.7296078, 'f1_score': 0.72912362889228, 'loss': 0.48787293}
Test Status: 
 {'accuracy': 0.75187504, 'precision': 0.7286597, 'recall': 0.72969437, 'f1_score': 0.7291766907003131, 'loss': 0.50601697}
